Spark are based on the concept of RDD , resilient distributed datasets , where the large data files are distributed over machines but on their 
hard disc but on their RAMs . 

spark over hadoop is chosen for various reasons . First being files in hadoop are on rom across machines , while files in spark are in ram / memory
across machines in cluster . So processing is in memory in case of spark and is fast . Second in hadoop , storage and computation are tightly 
coupled , so mapreduce processes files which are only on hdfs . While in spark , it has connectors with kafka , amazon s3 , cassandra , hadoop 
and it welcomes incoming data from all these sources ( batch / stream ) and processes it . so it has strategy of intermediate loading / storage 
and then processing it , be it multiple storage systems . Other point is that in hadoop , we have to process files in iteration , in passes that output
of one job getting sourced as input to anohter job , while in spark wiht help of functional apis , a stream of data can be iteratively processed
with functions , like rdd.map.reduceByKey . So a stream can be processed ( even a small discretized stream ) can be passed through and entire 
processing pipeline with help of chained functional apis . 

transformation is about creating a logical pipeline of operations while action is actual processing ( not logical ) . action marks the end of 
transformation pipeline and from there the entire transformation pipeline execution gets started till the finishing point of action , thereby 
returning a concrete value . example is rdd.map.reduceByKey , so here till rdd.map is transformation while .reduceByKey marks the action . 

each partition of kafka ( spread over cluster ) will be processed by unique executor/consumer and inside that multiple cores will 
spawn multiple jvms inside the cores which will run the map code and then will run reduce code .. and reduced result will be per executor 
in terms of part r files .  

-----------------------------------------------------------------------------------
non tuple style and non javaPairRdd 

JavaRDD<String> lines = sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(s -> s.length());
int totalLength = lineLengths.reduce((a, b) -> a + b);

so in above code maps run in parallel in executors of the machines and reducers are run locally 
also we wee that in map , input is singularly  the line and it's length is computed and return type is integer . 
return of map input which is a dataframe is stored in series of values stored in rdd of integer 
and then locally this rdd is computed via the reduce method ( action which forces data to be written to edge node ) where in 
iteratively two values of the rdd is passed , value sum is computed and subsequently total sum is computed and returned / written 
to the edge node . 


JavaRDD<String> lines = sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(new Function<String, Integer>() {
  public Integer call(String s) { return s.length(); }
});
int totalLength = lineLengths.reduce(new Function2<Integer, Integer, Integer>() {
  public Integer call(Integer a, Integer b) { return a + b; }
});

in the above code only difference is that in map function , a function is passed as an argument 
at first level input and output data types are mentioned for this entire passed method / function 
then the actual function with name , return type and parameters and method logic , like in a normal method is written  . 
in first case , we see input is the line and return type is integer which is the total of words in that line 
in second case we see that iteratively the rdd is processed by sending first two values and then summing it up and then doing 
it iteratively over the entire rdd , until the rdd ends 

------------------------------------------------------------------------------------------------------------
tuples style and javaPairRdd style 
  
  JavaRDD<String> lines = sc.textFile("data.txt");
JavaPairRDD<String, Integer> pairs = lines.mapToPair(s -> new Tuple2(s, 1)); // example apple , 1 ; boy ,1  , again in some line apple , 1
JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b); // apple , 2 ; boy , 1
We could also use counts.sortByKey(), for example, to sort the pairs alphabetically, and finally counts.collect() to bring them back to the driver program as an array of objects.
});





 
